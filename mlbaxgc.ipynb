{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import pandas\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(sys.argv)\n",
    "arr = []\n",
    "# Arguments passed\n",
    "for i in range(1, n):\n",
    "    arr.append(sys.argv[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kaggle_train.csv\", 'r') as temp_f:\n",
    "    # get No of columns in each line\n",
    "    col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n",
    "\n",
    "### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\n",
    "column_names = [i for i in range(0, max(col_count))]\n",
    "\n",
    "### Read csv\n",
    "df_train = pd.read_csv(\"kaggle_train.csv\", header=None, delimiter=\",\", names=column_names)\n",
    "\n",
    "df_train = df_train.replace(np.nan, 0)\n",
    "# df_dipeptide = df_dipeptide.values\n",
    "\n",
    "df_train = df_train[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13.6584</td>\n",
       "      <td>88.0205</td>\n",
       "      <td>402.542</td>\n",
       "      <td>1424.2625</td>\n",
       "      <td>3.794</td>\n",
       "      <td>1001.233</td>\n",
       "      <td>508.0148</td>\n",
       "      <td>567.201</td>\n",
       "      <td>13.6584</td>\n",
       "      <td>...</td>\n",
       "      <td>2437.6363</td>\n",
       "      <td>3413.2107</td>\n",
       "      <td>4135.4453</td>\n",
       "      <td>950.0104</td>\n",
       "      <td>1434.1269</td>\n",
       "      <td>0</td>\n",
       "      <td>31.8695</td>\n",
       "      <td>923.0769</td>\n",
       "      <td>1046.0021</td>\n",
       "      <td>819.1217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0844</td>\n",
       "      <td>86.3807</td>\n",
       "      <td>690.4861</td>\n",
       "      <td>2226.3285</td>\n",
       "      <td>20.4071</td>\n",
       "      <td>486.9744</td>\n",
       "      <td>1448.064</td>\n",
       "      <td>651.9083</td>\n",
       "      <td>11.182</td>\n",
       "      <td>...</td>\n",
       "      <td>2153.9253</td>\n",
       "      <td>1817.3762</td>\n",
       "      <td>3426.4325</td>\n",
       "      <td>1380.1196</td>\n",
       "      <td>815.7241</td>\n",
       "      <td>0</td>\n",
       "      <td>1651.0166</td>\n",
       "      <td>1473.7825</td>\n",
       "      <td>1618.8684</td>\n",
       "      <td>615.8465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>32.7422</td>\n",
       "      <td>53.082</td>\n",
       "      <td>128.4882</td>\n",
       "      <td>1073.0497</td>\n",
       "      <td>0</td>\n",
       "      <td>654.8431</td>\n",
       "      <td>492.6206</td>\n",
       "      <td>512.9604</td>\n",
       "      <td>13.8906</td>\n",
       "      <td>...</td>\n",
       "      <td>3122.411</td>\n",
       "      <td>3720.6995</td>\n",
       "      <td>3000.3721</td>\n",
       "      <td>2681.3791</td>\n",
       "      <td>1429.2447</td>\n",
       "      <td>0.4961</td>\n",
       "      <td>693.5384</td>\n",
       "      <td>477.2417</td>\n",
       "      <td>1064.6161</td>\n",
       "      <td>804.1672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.7975</td>\n",
       "      <td>42.1175</td>\n",
       "      <td>634.0804</td>\n",
       "      <td>3285.5487</td>\n",
       "      <td>0</td>\n",
       "      <td>574.9614</td>\n",
       "      <td>513.9104</td>\n",
       "      <td>1033.2303</td>\n",
       "      <td>31.6847</td>\n",
       "      <td>...</td>\n",
       "      <td>2663.4467</td>\n",
       "      <td>3217.9212</td>\n",
       "      <td>4131.3756</td>\n",
       "      <td>1178.864</td>\n",
       "      <td>1018.5471</td>\n",
       "      <td>0</td>\n",
       "      <td>675.0386</td>\n",
       "      <td>1596.5997</td>\n",
       "      <td>1666.1515</td>\n",
       "      <td>402.6275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>134.0625</td>\n",
       "      <td>532.1875</td>\n",
       "      <td>936.5625</td>\n",
       "      <td>0</td>\n",
       "      <td>1933.4375</td>\n",
       "      <td>679.375</td>\n",
       "      <td>1175.625</td>\n",
       "      <td>6.5625</td>\n",
       "      <td>...</td>\n",
       "      <td>1898.4375</td>\n",
       "      <td>3009.1375</td>\n",
       "      <td>2549.375</td>\n",
       "      <td>1293.1156</td>\n",
       "      <td>744.6875</td>\n",
       "      <td>0</td>\n",
       "      <td>69.0625</td>\n",
       "      <td>1651.5625</td>\n",
       "      <td>1250.3125</td>\n",
       "      <td>982.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1</td>\n",
       "      <td>61.3497</td>\n",
       "      <td>209.2706</td>\n",
       "      <td>300.6135</td>\n",
       "      <td>676.8916</td>\n",
       "      <td>4.09</td>\n",
       "      <td>866.394</td>\n",
       "      <td>836.4008</td>\n",
       "      <td>903.8855</td>\n",
       "      <td>40.8998</td>\n",
       "      <td>...</td>\n",
       "      <td>2700.7498</td>\n",
       "      <td>2858.8957</td>\n",
       "      <td>1663.94</td>\n",
       "      <td>2168.364</td>\n",
       "      <td>1858.214</td>\n",
       "      <td>0</td>\n",
       "      <td>441.0361</td>\n",
       "      <td>586.9121</td>\n",
       "      <td>583.5037</td>\n",
       "      <td>732.1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1</td>\n",
       "      <td>25.0447</td>\n",
       "      <td>89.7436</td>\n",
       "      <td>1050.3876</td>\n",
       "      <td>1677.6983</td>\n",
       "      <td>0</td>\n",
       "      <td>1019.0817</td>\n",
       "      <td>2058.1395</td>\n",
       "      <td>2086.4639</td>\n",
       "      <td>3.876</td>\n",
       "      <td>...</td>\n",
       "      <td>1367.0244</td>\n",
       "      <td>2176.7352</td>\n",
       "      <td>2595.4085</td>\n",
       "      <td>1222.412</td>\n",
       "      <td>2648.4794</td>\n",
       "      <td>0</td>\n",
       "      <td>471.9738</td>\n",
       "      <td>1548.5987</td>\n",
       "      <td>1888.4914</td>\n",
       "      <td>788.6106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0</td>\n",
       "      <td>18.8321</td>\n",
       "      <td>50.5493</td>\n",
       "      <td>631.0399</td>\n",
       "      <td>1997.8525</td>\n",
       "      <td>0.3304</td>\n",
       "      <td>821.6734</td>\n",
       "      <td>941.9344</td>\n",
       "      <td>912.1996</td>\n",
       "      <td>17.8409</td>\n",
       "      <td>...</td>\n",
       "      <td>2085.7355</td>\n",
       "      <td>2405.6562</td>\n",
       "      <td>3461.1382</td>\n",
       "      <td>1596.0981</td>\n",
       "      <td>665.7306</td>\n",
       "      <td>0.3304</td>\n",
       "      <td>1272.3218</td>\n",
       "      <td>763.8556</td>\n",
       "      <td>1522.425</td>\n",
       "      <td>347.2371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0</td>\n",
       "      <td>17.2414</td>\n",
       "      <td>92.81</td>\n",
       "      <td>825.3852</td>\n",
       "      <td>2262.6559</td>\n",
       "      <td>0.3668</td>\n",
       "      <td>1421.8635</td>\n",
       "      <td>960.7483</td>\n",
       "      <td>1231.1079</td>\n",
       "      <td>2.9347</td>\n",
       "      <td>...</td>\n",
       "      <td>2774.0279</td>\n",
       "      <td>2888.9142</td>\n",
       "      <td>4561.6288</td>\n",
       "      <td>1594.2296</td>\n",
       "      <td>653.7051</td>\n",
       "      <td>0</td>\n",
       "      <td>392.5165</td>\n",
       "      <td>2422.964</td>\n",
       "      <td>1749.0829</td>\n",
       "      <td>768.1585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0</td>\n",
       "      <td>30.6291</td>\n",
       "      <td>96.0265</td>\n",
       "      <td>541.6667</td>\n",
       "      <td>2443.4327</td>\n",
       "      <td>0.2759</td>\n",
       "      <td>2218.5403</td>\n",
       "      <td>1235.9272</td>\n",
       "      <td>961.6446</td>\n",
       "      <td>4.415</td>\n",
       "      <td>...</td>\n",
       "      <td>2553.8079</td>\n",
       "      <td>2839.9558</td>\n",
       "      <td>4388.245</td>\n",
       "      <td>1104.2908</td>\n",
       "      <td>602.649</td>\n",
       "      <td>0</td>\n",
       "      <td>411.4238</td>\n",
       "      <td>835.5408</td>\n",
       "      <td>1103.2009</td>\n",
       "      <td>713.3002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows Ã— 319 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0        1         2          3          4        5          6     \n",
       "1     0  13.6584   88.0205    402.542  1424.2625    3.794   1001.233  \\\n",
       "2     0  22.0844   86.3807   690.4861  2226.3285  20.4071   486.9744   \n",
       "3     0  32.7422    53.082   128.4882  1073.0497        0   654.8431   \n",
       "4     0  22.7975   42.1175   634.0804  3285.5487        0   574.9614   \n",
       "5     0      2.5  134.0625   532.1875   936.5625        0  1933.4375   \n",
       "..   ..      ...       ...        ...        ...      ...        ...   \n",
       "248   1  61.3497  209.2706   300.6135   676.8916     4.09    866.394   \n",
       "249   1  25.0447   89.7436  1050.3876  1677.6983        0  1019.0817   \n",
       "250   0  18.8321   50.5493   631.0399  1997.8525   0.3304   821.6734   \n",
       "251   0  17.2414     92.81   825.3852  2262.6559   0.3668  1421.8635   \n",
       "252   0  30.6291   96.0265   541.6667  2443.4327   0.2759  2218.5403   \n",
       "\n",
       "           7          8        9    ...        309        310        311   \n",
       "1     508.0148    567.201  13.6584  ...  2437.6363  3413.2107  4135.4453  \\\n",
       "2     1448.064   651.9083   11.182  ...  2153.9253  1817.3762  3426.4325   \n",
       "3     492.6206   512.9604  13.8906  ...   3122.411  3720.6995  3000.3721   \n",
       "4     513.9104  1033.2303  31.6847  ...  2663.4467  3217.9212  4131.3756   \n",
       "5      679.375   1175.625   6.5625  ...  1898.4375  3009.1375   2549.375   \n",
       "..         ...        ...      ...  ...        ...        ...        ...   \n",
       "248   836.4008   903.8855  40.8998  ...  2700.7498  2858.8957    1663.94   \n",
       "249  2058.1395  2086.4639    3.876  ...  1367.0244  2176.7352  2595.4085   \n",
       "250   941.9344   912.1996  17.8409  ...  2085.7355  2405.6562  3461.1382   \n",
       "251   960.7483  1231.1079   2.9347  ...  2774.0279  2888.9142  4561.6288   \n",
       "252  1235.9272   961.6446    4.415  ...  2553.8079  2839.9558   4388.245   \n",
       "\n",
       "           312        313     314        315        316        317       318  \n",
       "1     950.0104  1434.1269       0    31.8695   923.0769  1046.0021  819.1217  \n",
       "2    1380.1196   815.7241       0  1651.0166  1473.7825  1618.8684  615.8465  \n",
       "3    2681.3791  1429.2447  0.4961   693.5384   477.2417  1064.6161  804.1672  \n",
       "4     1178.864  1018.5471       0   675.0386  1596.5997  1666.1515  402.6275  \n",
       "5    1293.1156   744.6875       0    69.0625  1651.5625  1250.3125     982.5  \n",
       "..         ...        ...     ...        ...        ...        ...       ...  \n",
       "248   2168.364   1858.214       0   441.0361   586.9121   583.5037  732.1063  \n",
       "249   1222.412  2648.4794       0   471.9738  1548.5987  1888.4914  788.6106  \n",
       "250  1596.0981   665.7306  0.3304  1272.3218   763.8556   1522.425  347.2371  \n",
       "251  1594.2296   653.7051       0   392.5165   2422.964  1749.0829  768.1585  \n",
       "252  1104.2908    602.649       0   411.4238   835.5408  1103.2009  713.3002  \n",
       "\n",
       "[252 rows x 319 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kaggle_test.csv\", 'r') as temp_f:\n",
    "    # get No of columns in each line\n",
    "    col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n",
    "\n",
    "### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\n",
    "column_names = [i for i in range(0, max(col_count))]\n",
    "\n",
    "### Read csv\n",
    "df_test = pd.read_csv(\"kaggle_test.csv\", header=None, delimiter=\",\", names=column_names)\n",
    "\n",
    "df_test = df_test.replace(np.nan, 0)\n",
    "df_test = df_test[1:].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252, 318)\n",
      "(252,)\n"
     ]
    }
   ],
   "source": [
    "# Getting Values into x_train, y_train, x_test and y_test variables and checking their final values\n",
    "\n",
    "y_train = df_train.values[:,0].astype('int')\n",
    "x_train = df_train.values[:,1:].astype('float')\n",
    "\n",
    "# Size fo x _train and y_train\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([124, 128], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  13.6584,   88.0205,  402.542 , ...,  923.0769, 1046.0021,\n",
       "         819.1217],\n",
       "       [  22.0844,   86.3807,  690.4861, ..., 1473.7825, 1618.8684,\n",
       "         615.8465],\n",
       "       [  32.7422,   53.082 ,  128.4882, ...,  477.2417, 1064.6161,\n",
       "         804.1672],\n",
       "       ...,\n",
       "       [  18.8321,   50.5493,  631.0399, ...,  763.8556, 1522.425 ,\n",
       "         347.2371],\n",
       "       [  17.2414,   92.81  ,  825.3852, ..., 2422.964 , 1749.0829,\n",
       "         768.1585],\n",
       "       [  30.6291,   96.0265,  541.6667, ...,  835.5408, 1103.2009,\n",
       "         713.3002]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 318)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dipeptides for testing data set \n",
    "x_test = df_test[:, 1:].astype('float')\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((252, 318), (104, 318))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "318\n",
      "Top features:\n",
      "1. feature 68 (0.018788)\n",
      "2. feature 199 (0.011590)\n",
      "3. feature 159 (0.011079)\n",
      "4. feature 294 (0.010793)\n",
      "5. feature 236 (0.010156)\n",
      "6. feature 314 (0.010014)\n",
      "7. feature 204 (0.009999)\n",
      "8. feature 266 (0.009979)\n",
      "9. feature 119 (0.009876)\n",
      "10. feature 267 (0.009741)\n",
      "11. feature 201 (0.009617)\n",
      "12. feature 232 (0.008398)\n",
      "13. feature 315 (0.008318)\n",
      "14. feature 74 (0.008159)\n",
      "15. feature 47 (0.007882)\n",
      "16. feature 210 (0.007878)\n",
      "17. feature 205 (0.007864)\n",
      "18. feature 141 (0.007476)\n",
      "19. feature 312 (0.007405)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.415736\n",
      "1      0.377616\n",
      "2      0.578313\n",
      "3      0.372309\n",
      "4      0.422095\n",
      "         ...   \n",
      "99     0.438208\n",
      "100    0.642841\n",
      "101    0.235179\n",
      "102    0.614380\n",
      "103    0.312501\n",
      "Length: 104, dtype: float64\n",
      "19\n",
      "318\n",
      "Top features:\n",
      "1. feature 68 (0.017718)\n",
      "2. feature 210 (0.012540)\n",
      "3. feature 158 (0.011155)\n",
      "4. feature 171 (0.010353)\n",
      "5. feature 301 (0.010285)\n",
      "6. feature 93 (0.009402)\n",
      "7. feature 46 (0.009124)\n",
      "8. feature 60 (0.009097)\n",
      "9. feature 314 (0.008811)\n",
      "10. feature 312 (0.008733)\n",
      "11. feature 294 (0.008412)\n",
      "12. feature 308 (0.008018)\n",
      "13. feature 76 (0.007810)\n",
      "14. feature 19 (0.007300)\n",
      "15. feature 209 (0.007293)\n",
      "16. feature 231 (0.007284)\n",
      "17. feature 47 (0.007070)\n",
      "18. feature 86 (0.007008)\n",
      "19. feature 266 (0.006960)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.377464\n",
      "1      0.450327\n",
      "2      0.489172\n",
      "3      0.564345\n",
      "4      0.310975\n",
      "         ...   \n",
      "99     0.475756\n",
      "100    0.571674\n",
      "101    0.332198\n",
      "102    0.639360\n",
      "103    0.246934\n",
      "Length: 104, dtype: float64\n",
      "19\n",
      "318\n",
      "Top features:\n",
      "1. feature 68 (0.012558)\n",
      "2. feature 201 (0.010990)\n",
      "3. feature 312 (0.010597)\n",
      "4. feature 171 (0.010536)\n",
      "5. feature 264 (0.009862)\n",
      "6. feature 158 (0.009854)\n",
      "7. feature 314 (0.009526)\n",
      "8. feature 210 (0.009357)\n",
      "9. feature 294 (0.008914)\n",
      "10. feature 200 (0.008387)\n",
      "11. feature 153 (0.008313)\n",
      "12. feature 301 (0.008226)\n",
      "13. feature 159 (0.008118)\n",
      "14. feature 267 (0.007900)\n",
      "15. feature 236 (0.007848)\n",
      "16. feature 308 (0.007840)\n",
      "17. feature 72 (0.007418)\n",
      "18. feature 199 (0.007276)\n",
      "19. feature 60 (0.007204)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.327707\n",
      "1      0.388970\n",
      "2      0.610590\n",
      "3      0.405751\n",
      "4      0.413534\n",
      "         ...   \n",
      "99     0.464971\n",
      "100    0.677692\n",
      "101    0.297344\n",
      "102    0.596726\n",
      "103    0.325838\n",
      "Length: 104, dtype: float64\n",
      "19\n",
      "318\n",
      "Top features:\n",
      "1. feature 68 (0.013357)\n",
      "2. feature 171 (0.012141)\n",
      "3. feature 201 (0.011721)\n",
      "4. feature 314 (0.011212)\n",
      "5. feature 210 (0.011049)\n",
      "6. feature 60 (0.010227)\n",
      "7. feature 158 (0.010112)\n",
      "8. feature 267 (0.009795)\n",
      "9. feature 205 (0.009442)\n",
      "10. feature 232 (0.008057)\n",
      "11. feature 57 (0.007971)\n",
      "12. feature 58 (0.007819)\n",
      "13. feature 263 (0.007595)\n",
      "14. feature 238 (0.007534)\n",
      "15. feature 312 (0.007443)\n",
      "16. feature 184 (0.007295)\n",
      "17. feature 77 (0.007268)\n",
      "18. feature 164 (0.007251)\n",
      "19. feature 206 (0.007165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.537654\n",
      "1      0.657107\n",
      "2      0.450774\n",
      "3      0.317154\n",
      "4      0.449418\n",
      "         ...   \n",
      "99     0.513457\n",
      "100    0.624934\n",
      "101    0.353353\n",
      "102    0.616165\n",
      "103    0.238365\n",
      "Length: 104, dtype: float64\n",
      "19\n",
      "318\n",
      "Top features:\n",
      "1. feature 158 (0.012725)\n",
      "2. feature 93 (0.012485)\n",
      "3. feature 232 (0.011871)\n",
      "4. feature 68 (0.011658)\n",
      "5. feature 294 (0.010478)\n",
      "6. feature 201 (0.010002)\n",
      "7. feature 209 (0.009857)\n",
      "8. feature 314 (0.009379)\n",
      "9. feature 264 (0.009365)\n",
      "10. feature 210 (0.009278)\n",
      "11. feature 238 (0.008862)\n",
      "12. feature 205 (0.008820)\n",
      "13. feature 153 (0.008799)\n",
      "14. feature 60 (0.007890)\n",
      "15. feature 121 (0.007830)\n",
      "16. feature 171 (0.007715)\n",
      "17. feature 267 (0.007543)\n",
      "18. feature 39 (0.007111)\n",
      "19. feature 58 (0.007111)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.341812\n",
      "1      0.620468\n",
      "2      0.649996\n",
      "3      0.361692\n",
      "4      0.517531\n",
      "         ...   \n",
      "99     0.461359\n",
      "100    0.625437\n",
      "101    0.317577\n",
      "102    0.601203\n",
      "103    0.238979\n",
      "Length: 104, dtype: float64\n",
      "19\n",
      "318\n",
      "Top features:\n",
      "1. feature 68 (0.013562)\n",
      "2. feature 232 (0.011511)\n",
      "3. feature 171 (0.011236)\n",
      "4. feature 158 (0.010819)\n",
      "5. feature 209 (0.010683)\n",
      "6. feature 267 (0.009565)\n",
      "7. feature 201 (0.009309)\n",
      "8. feature 264 (0.009019)\n",
      "9. feature 314 (0.008971)\n",
      "10. feature 47 (0.008716)\n",
      "11. feature 60 (0.008644)\n",
      "12. feature 231 (0.008178)\n",
      "13. feature 58 (0.008053)\n",
      "14. feature 95 (0.008012)\n",
      "15. feature 57 (0.007892)\n",
      "16. feature 312 (0.007814)\n",
      "17. feature 236 (0.007744)\n",
      "18. feature 153 (0.007574)\n",
      "19. feature 82 (0.007451)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.334447\n",
      "1      0.532867\n",
      "2      0.634718\n",
      "3      0.424723\n",
      "4      0.442693\n",
      "         ...   \n",
      "99     0.440291\n",
      "100    0.524582\n",
      "101    0.267930\n",
      "102    0.630779\n",
      "103    0.240808\n",
      "Length: 104, dtype: float64\n",
      "19\n",
      "318\n",
      "Top features:\n",
      "1. feature 68 (0.013757)\n",
      "2. feature 171 (0.012260)\n",
      "3. feature 159 (0.011377)\n",
      "4. feature 314 (0.009994)\n",
      "5. feature 301 (0.009792)\n",
      "6. feature 200 (0.009519)\n",
      "7. feature 267 (0.009470)\n",
      "8. feature 199 (0.009466)\n",
      "9. feature 210 (0.008972)\n",
      "10. feature 278 (0.008796)\n",
      "11. feature 209 (0.008571)\n",
      "12. feature 201 (0.008491)\n",
      "13. feature 153 (0.008465)\n",
      "14. feature 205 (0.008387)\n",
      "15. feature 158 (0.007853)\n",
      "16. feature 93 (0.007852)\n",
      "17. feature 236 (0.007804)\n",
      "18. feature 308 (0.007490)\n",
      "19. feature 60 (0.007472)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.368954\n",
      "1      0.488198\n",
      "2      0.513131\n",
      "3      0.429263\n",
      "4      0.290223\n",
      "         ...   \n",
      "99     0.458460\n",
      "100    0.657382\n",
      "101    0.289530\n",
      "102    0.611957\n",
      "103    0.295476\n",
      "Length: 104, dtype: float64\n",
      "19\n",
      "318\n",
      "Top features:\n",
      "1. feature 68 (0.020448)\n",
      "2. feature 210 (0.011954)\n",
      "3. feature 60 (0.010757)\n",
      "4. feature 200 (0.009107)\n",
      "5. feature 153 (0.008681)\n",
      "6. feature 206 (0.008219)\n",
      "7. feature 199 (0.007941)\n",
      "8. feature 301 (0.007890)\n",
      "9. feature 267 (0.007887)\n",
      "10. feature 158 (0.007871)\n",
      "11. feature 113 (0.007813)\n",
      "12. feature 272 (0.007703)\n",
      "13. feature 204 (0.007684)\n",
      "14. feature 312 (0.007584)\n",
      "15. feature 238 (0.007373)\n",
      "16. feature 171 (0.007346)\n",
      "17. feature 205 (0.007145)\n",
      "18. feature 39 (0.007050)\n",
      "19. feature 38 (0.006910)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.443673\n",
      "1      0.561669\n",
      "2      0.602144\n",
      "3      0.520557\n",
      "4      0.305142\n",
      "         ...   \n",
      "99     0.429647\n",
      "100    0.668700\n",
      "101    0.307026\n",
      "102    0.574881\n",
      "103    0.349507\n",
      "Length: 104, dtype: float64\n",
      "19\n",
      "318\n",
      "Top features:\n",
      "1. feature 158 (0.014459)\n",
      "2. feature 209 (0.013252)\n",
      "3. feature 314 (0.010413)\n",
      "4. feature 267 (0.009869)\n",
      "5. feature 68 (0.008621)\n",
      "6. feature 210 (0.008518)\n",
      "7. feature 272 (0.008153)\n",
      "8. feature 58 (0.007823)\n",
      "9. feature 301 (0.007794)\n",
      "10. feature 236 (0.007745)\n",
      "11. feature 249 (0.007541)\n",
      "12. feature 294 (0.007340)\n",
      "13. feature 199 (0.007304)\n",
      "14. feature 232 (0.007242)\n",
      "15. feature 200 (0.007218)\n",
      "16. feature 201 (0.007064)\n",
      "17. feature 171 (0.007007)\n",
      "18. feature 77 (0.007005)\n",
      "19. feature 238 (0.006799)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.384832\n",
      "1      0.555707\n",
      "2      0.457458\n",
      "3      0.566118\n",
      "4      0.436516\n",
      "         ...   \n",
      "99     0.397306\n",
      "100    0.671043\n",
      "101    0.315598\n",
      "102    0.632812\n",
      "103    0.237719\n",
      "Length: 104, dtype: float64\n",
      "19\n",
      "318\n",
      "Top features:\n",
      "1. feature 68 (0.018788)\n",
      "2. feature 199 (0.011590)\n",
      "3. feature 159 (0.011079)\n",
      "4. feature 294 (0.010793)\n",
      "5. feature 236 (0.010156)\n",
      "6. feature 314 (0.010014)\n",
      "7. feature 204 (0.009999)\n",
      "8. feature 266 (0.009979)\n",
      "9. feature 119 (0.009876)\n",
      "10. feature 267 (0.009741)\n",
      "11. feature 201 (0.009617)\n",
      "12. feature 232 (0.008398)\n",
      "13. feature 315 (0.008318)\n",
      "14. feature 74 (0.008159)\n",
      "15. feature 47 (0.007882)\n",
      "16. feature 210 (0.007878)\n",
      "17. feature 205 (0.007864)\n",
      "18. feature 141 (0.007476)\n",
      "19. feature 312 (0.007405)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.420076\n",
      "1      0.386825\n",
      "2      0.575675\n",
      "3      0.371511\n",
      "4      0.427560\n",
      "         ...   \n",
      "99     0.452457\n",
      "100    0.644399\n",
      "101    0.235365\n",
      "102    0.613342\n",
      "103    0.305319\n",
      "Length: 104, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "for rs in [1, 2, 3, 4, 5, 6, 7, 8, 9, 1]:\n",
    "# TOP_FEATURES = 15\n",
    "\n",
    "  forest = ExtraTreesClassifier(n_estimators=250, max_depth=5, random_state=rs)\n",
    "  forest.fit(x_train, y_train)\n",
    "\n",
    "  importances = forest.feature_importances_\n",
    "  std = np.std(\n",
    "      [tree.feature_importances_ for tree in forest.estimators_],\n",
    "      axis=0\n",
    "  )\n",
    "  indices = np.argsort(importances)[::-1]\n",
    "  indices = indices[:19]\n",
    "  print(len(indices))\n",
    "  print(len(importances))\n",
    "  print('Top features:')\n",
    "  list = []\n",
    "  for f in range(19):\n",
    "    list.append(indices[f])\n",
    "    print('%d. feature %d (%f)' % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "  x_train_2 = x_train[:,list]\n",
    "  x_test_2 = x_test[:,list]\n",
    "\n",
    "  models = [\n",
    "      LogisticRegression(),\n",
    "      XGBClassifier(max_depth=2),\n",
    "      GradientBoostingClassifier(learning_rate=0.001, n_estimators=300),\n",
    "      RandomForestClassifier(max_depth=2, max_features='log2', n_estimators=50),\n",
    "\n",
    "  ]\n",
    "\n",
    "  preds = pd.DataFrame()\n",
    "  for i, m in enumerate(models):\n",
    "      m.fit(x_train_2, y_train),\n",
    "      preds[i] = m.predict_proba(x_test_2)[:,1]\n",
    "\n",
    "  # weights = [1, 0.8, 0.6, 0.4]\n",
    "  weights = [1, 0.8, 0.8, 0.6]\n",
    "  # weights = [1, 0.8, 0.8, 0.8]\n",
    "  # weights = [1, 0.8, 0.4, 0.6]\n",
    "  y_test_pred= (preds * weights).sum(axis=1) / sum(weights)\n",
    "  print(y_test_pred)\n",
    "\n",
    "  # final array for output\n",
    "  y_final = ['Labels']\n",
    "\n",
    "  for i in range(0, len(y_test_pred)):\n",
    "    y_final.append((y_test_pred[i]))\n",
    "\n",
    "  # dataframe to store the final output\n",
    "  df2 = pd.read_csv(\"kaggle_test.csv\", header = None).loc[:, :2]\n",
    "  df2[2] = y_final[:]\n",
    "  df2 = df2.drop(1, axis = 1)\n",
    "  df2 = df2.values\n",
    "\n",
    "  #Saving the output file\n",
    "  pd.DataFrame(df2).to_csv(\"result\"+str(rs)+\".csv\", header = None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
